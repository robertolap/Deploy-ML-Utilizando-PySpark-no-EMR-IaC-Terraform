# Implementation and Deployment of Machine Learning with PySpark on Amazon EMR
The main goal of the project is to implement and deploy an advanced distributed Machine Learning training stack using PySpark on Amazon Elastic MapReduce (EMR). The idea is to leverage distributed processing to train large-scale Machine Learning models, optimizing resource use and reducing training time. The focus is on the ease of setting up an AWS infrastructure with Terraform code; ML was just a use case that can be applied.

## Objectives
- Utilize PySpark, a Python interface for Apache Spark, to handle common Big Data challenges, allowing complex analyses and efficient processing of large data volumes.

- Set up an EMR cluster on AWS to run distributed Machine Learning tasks.

- Design a scalable and resilient training stack, leveraging AWS services for resource management and monitoring.

- Facilitate automated deployment and continuous integration through scripts and infrastructure as code templates, ensuring environment replicability and consistency.

## Key Components
- Configuration of an EMR cluster on AWS.

- Utilization of PySpark for distributed training of Machine Learning models.

- Implementation of best practices in Data Science, including data preparation, model selection, and performance evaluation in a distributed context.

## Data Source
The data used in the project was prepared based on the data available at the following link: Stanford AI Group - Sentiment Analysis

This project aims to provide a robust solution for training Machine Learning models capable of handling petabyte-scale data challenges, making it ideal for organizations seeking deep insights from large datasets.
